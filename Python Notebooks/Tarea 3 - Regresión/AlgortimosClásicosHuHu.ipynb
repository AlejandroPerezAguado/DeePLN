{"cells":[{"cell_type":"markdown","metadata":{"id":"e_Fo89pZAHZw"},"source":["Enfoque del dataset de noticias falsas basado en algoritmos clásicos de aprendizaje automático"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15359,"status":"ok","timestamp":1684489759388,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"6p1Sno_M_bhI","outputId":"6018de9c-f994-4097-bc4b-7800a7ada567"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: textaugment in /usr/local/lib/python3.10/dist-packages (1.3.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textaugment) (3.8.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from textaugment) (4.3.1)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from textaugment) (0.17.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from textaugment) (1.22.4)\n","Requirement already satisfied: googletrans in /usr/local/lib/python3.10/dist-packages (from textaugment) (3.0.0)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->textaugment) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->textaugment) (6.3.0)\n","Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans->textaugment) (0.13.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2022.12.7)\n","Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2023.1.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (1.3.0)\n","Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (3.0.4)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2.10)\n","Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (1.5.0)\n","Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (0.9.1)\n","Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (0.9.0)\n","Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (3.2.0)\n","Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (5.2.0)\n","Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (3.0.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (4.65.0)\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["### IMPORTS ###\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","#from keras.utils import to_categorical\n","\n","!pip install textaugment\n","from textaugment import EDA\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","import re\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_absolute_error\n","\n","from sklearn.linear_model import BayesianRidge\n","from sklearn.ensemble import RandomForestRegressor"]},{"cell_type":"markdown","metadata":{"id":"WbjNq11JgYqU"},"source":["# Cargar Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2341,"status":"ok","timestamp":1684489761704,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"2ohgLOFB5LmD","outputId":"4ca17cac-963e-483f-fd48-34594d78e019"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","PATH = \"/content/drive/My Drive/TFM/Data/Huhu/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1684489761705,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"X8Yhoi3y5MKQ","outputId":"71ed7296-253e-4171-de1e-413a86f32e8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tamaño conjunto de Entrenamiento: 2136\n","Tamaño conjunto de Evaluación: 535\n"]}],"source":["### PARTICIÓN ###\n","df = pd.read_csv(PATH + \"train.csv\",  sep=',', on_bad_lines='skip', encoding='utf-8', encoding_errors='ignore')\n","df = df[['tweet', 'mean_prejudice']]\n","df = df.rename(columns={\"tweet\": \"Text\", \"mean_prejudice\": \"Label\"})\n","df.fillna(\" \", inplace=True)\n","\n","X_train = df['Text']\n","y_train = df['Label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=55)\n","\n","X_train = X_train.reset_index(drop=True)\n","y_train = y_train.reset_index(drop=True)\n","\n","print('Tamaño conjunto de Entrenamiento:', len(X_train))\n","print('Tamaño conjunto de Evaluación:', len(X_test))"]},{"cell_type":"markdown","metadata":{"id":"OuwFj0heTKW6"},"source":["# Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"NHyRaWcjTKW6"},"source":["t = EDA()\n","\n","i = 0\n","for i in range(2136):\n","    new_text = t.synonym_replacement(X_train[i])\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","    \n","    new_text = t.random_swap(X_train[i])\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","    \n","    new_text = t.random_deletion(X_train[i])\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","    \n","    i = i + 1\n","    \n","print('Tamaño conjunto de Entrenamiento:', len(X_train))\n","print('Tamaño conjunto de Evaluación:', len(X_test)) "]},{"cell_type":"markdown","metadata":{"id":"eAc3QSGcgcdG"},"source":["# Label Encoding"]},{"cell_type":"markdown","metadata":{"id":"MFQRNM8cnQsV"},"source":["No es necesario hacer un encoding ya que viene etiquetado de manera numérica"]},{"cell_type":"markdown","metadata":{"id":"Y2cga-nDjtlO"},"source":["# Limpieza y Representación de Textos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"badgpuYujv9M"},"outputs":[],"source":["### LIMPIEZA DE TEXTOS ###\n","stopwords_es = stopwords.words(\"spanish\")\n","def clean_text(text):\n","    # transformar a minúscula\n","    text=str(text).lower()\n","    # tokenizar\n","    tokens=word_tokenize(text)\n","    # borrar stopwords\n","    tokens = [word for word in tokens if word not in stopwords_es]\n","    # usar los stems\n","    tokens = [PorterStemmer().stem(word) for word in tokens]\n","    # eliminamos las palabras con 1 carácter\n","    # ignoramos cualquier palabra que contenga un digito o un símbolo especial \n","    min_length = 1\n","    p = re.compile('^[a-zA-Z]+$');\n","    filtered_tokens=[]\n","    for token in tokens:\n","        if len(token)>=min_length and p.match(token):\n","            filtered_tokens.append(token)\n","            \n","    return filtered_tokens"]},{"cell_type":"markdown","metadata":{"id":"BB2aQGIftSjU"},"source":["# Bolsa de Palabras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5522,"status":"ok","timestamp":1684489768167,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"2LyyJkH4tU9K","outputId":"4f87d910-fa3a-4f4f-bf7f-aa127ab14534"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tamaño del vocabulario:  6373\n"]}],"source":["### BOLSA DE PALABRAS ###\n","X_train = X_train.tolist()\n","X_test = X_test.tolist()\n","\n","# entrenamos un modelo de bolsa de palabras\n","bow = CountVectorizer(analyzer=clean_text).fit(X_train)\n","# transformamos el conjunto de entrenamiento a bolsa de palabras\n","X_train_bow = bow.transform(X_train)\n","# transformamos el conjunto de evaluación a bolsa de palabras\n","X_test_bow = bow.transform(X_test)\n","\n","print(\"Tamaño del vocabulario: \", len(bow.vocabulary_))"]},{"cell_type":"markdown","metadata":{"id":"ESXpf0ZAvLAq"},"source":["# TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6kuJq4avM_g"},"outputs":[],"source":["### TF-IDF ###\n","# entrenamos un modelo tf-idf \n","tfidf_transformer = TfidfTransformer().fit(X_train_bow)\n","# transformamos el conjunto de entrenamiento\n","X_train_tfidf = tfidf_transformer.transform(X_train_bow)\n","# transformamos el conjunto de entrenamiento\n","X_test_tfidf = tfidf_transformer.transform(X_test_bow)"]},{"cell_type":"markdown","metadata":{"id":"24Zza6O4wC4q"},"source":["# Regresión Clásica\n","Se crea un pipeline que ejecuta una secuencia de procesos:\n","\n","\n","1.   La representación de los textos en bolsa de palabras (CountVectorizer), que recibe como entrada los textos, y se les aplica dentro del CountVectorizer la función clean_text para limpiarlos y reducir el ruido. \n","2.   La representación en tf-idf (TfidfTransformer), recibe como entrada la salida del proceso 1, y produce los vectores tf-idf. \n","3. El clasificador SVC, Logistic Regression o Random Forest Clasiffier."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89384,"status":"ok","timestamp":1684489857545,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"-vlh4ZNrwFcH","outputId":"bca36fd8-3701-452e-87c9-fe31bab04dc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 4 folds for each of 8 candidates, totalling 32 fits\n","Los mejores parámetros son : {'svm__C': 1, 'svm__gamma': 1, 'svm__kernel': 'rbf'}\n","Mejor score: 0.208\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7f0ee05e3eb0>)),\n","                ('tf', TfidfTransformer()), ('svm', SVR(C=1, gamma=1))])\n","0.5497910267534595\n"]}],"source":["### PIPELINE SVR ###\n","pipeline = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),  \n","    ('tf', TfidfTransformer()),  \n","    ('svm', SVR()), \n","])\n","\n","# Parámetros para el algoritmo SVM\n","grid_params_svm = [{'svm__kernel': ['linear', 'rbf'], \n","                    'svm__C': [0.1, 1], # [0.1, 1, 10, 100, 1000]\n","                    'svm__gamma':  [1, 0.1] # [1, 0.1, 0.01, 0.001, 0.0001]\n","                    }]\n","gs = GridSearchCV(pipeline, param_grid=grid_params_svm, cv=4, verbose = 1)\n","\n","# entrenamos el grid\n","gs.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs.best_params_)\n","print('Mejor score: %.3f' % gs.best_score_)\n","print(gs.best_estimator_)\n","\n","best_svm = gs.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print(mean_absolute_error(y_test, predictions))"]}],"metadata":{"accelerator":"TPU","colab":{"toc_visible":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}