{"cells":[{"cell_type":"markdown","metadata":{"id":"e_Fo89pZAHZw"},"source":["Enfoque del dataset de noticias falsas basado en algoritmos clásicos de aprendizaje automático"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8954,"status":"ok","timestamp":1683817270631,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"6p1Sno_M_bhI","outputId":"9b15bc52-296b-4955-c1a0-aab5ebde2766"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: textaugment in /usr/local/lib/python3.10/dist-packages (1.3.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textaugment) (3.8.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from textaugment) (4.3.1)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from textaugment) (0.17.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from textaugment) (1.22.4)\n","Requirement already satisfied: googletrans in /usr/local/lib/python3.10/dist-packages (from textaugment) (3.0.0)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->textaugment) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->textaugment) (6.3.0)\n","Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans->textaugment) (0.13.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2022.12.7)\n","Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2023.1.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (1.3.0)\n","Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (3.0.4)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2.10)\n","Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (1.5.0)\n","Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (0.9.1)\n","Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (0.9.0)\n","Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (3.2.0)\n","Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (5.2.0)\n","Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (3.0.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (4.65.0)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["### IMPORTS ###\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","#from keras.utils import to_categorical\n","\n","!pip install textaugment\n","from textaugment import EDA\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","import re\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"markdown","metadata":{"id":"WbjNq11JgYqU"},"source":["# Cargar Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29052,"status":"ok","timestamp":1683817299669,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"2ohgLOFB5LmD","outputId":"fcff6a4f-334d-4fab-8d8e-16f96d1a47ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","PATH = \"/content/drive/My Drive/TFM/Data/Huhu/\""]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1656,"status":"ok","timestamp":1683817301321,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"X8Yhoi3y5MKQ","outputId":"5822fae2-b319-43e8-ec25-8e999b671493"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño conjunto de Entrenamiento: 2136\n","Tamaño conjunto de Evaluación: 535\n"]}],"source":["### PARTICIÓN ###\n","df = pd.read_csv(PATH + \"train.csv\",  sep=',', on_bad_lines='skip', encoding='utf-8', encoding_errors='ignore')\n","df = df[['tweet', 'humor']]\n","df = df.rename(columns={\"tweet\": \"Text\", \"humor\": \"Label\"})\n","df.fillna(\" \", inplace=True)\n","\n","X_train = df['Text']\n","y_train = df['Label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=55, stratify=y_train)\n","\n","X_train = X_train.reset_index(drop=True)\n","y_train = y_train.reset_index(drop=True)\n","\n","print('Tamaño conjunto de Entrenamiento:', len(X_train))\n","print('Tamaño conjunto de Evaluación:', len(X_test))"]},{"cell_type":"markdown","metadata":{"id":"OuwFj0heTKW6"},"source":["# Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"NHyRaWcjTKW6"},"source":["t = EDA()\n","\n","i = 0\n","for i in range(2136):\n","    new_text = t.synonym_replacement(X_train[i])\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","    \n","    new_text = t.random_swap(X_train[i])\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","    \n","    new_text = t.random_deletion(X_train[i])\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","    \n","    i = i + 1\n","    \n","print('Tamaño conjunto de Entrenamiento:', len(X_train))\n","print('Tamaño conjunto de Evaluación:', len(X_test)) "]},{"cell_type":"markdown","metadata":{"id":"eAc3QSGcgcdG"},"source":["# Label Encoding"]},{"cell_type":"markdown","metadata":{"id":"MFQRNM8cnQsV"},"source":["No es necesario hacer un encoding ya que viene etiquetado de manera numérica"]},{"cell_type":"markdown","metadata":{"id":"Y2cga-nDjtlO"},"source":["# Limpieza y Representación de Textos"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1683817301323,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"badgpuYujv9M"},"outputs":[],"source":["### LIMPIEZA DE TEXTOS ###\n","stopwords_es = stopwords.words(\"spanish\")\n","def clean_text(text):\n","    # transformar a minúscula\n","    text=str(text).lower()\n","    # tokenizar\n","    tokens=word_tokenize(text)\n","    # borrar stopwords\n","    tokens = [word for word in tokens if word not in stopwords_es]\n","    # usar los stems\n","    tokens = [PorterStemmer().stem(word) for word in tokens]\n","    # eliminamos las palabras con 1 carácter\n","    # ignoramos cualquier palabra que contenga un digito o un símbolo especial \n","    min_length = 1\n","    p = re.compile('^[a-zA-Z]+$');\n","    filtered_tokens=[]\n","    for token in tokens:\n","        if len(token)>=min_length and p.match(token):\n","            filtered_tokens.append(token)\n","            \n","    return filtered_tokens"]},{"cell_type":"markdown","metadata":{"id":"BB2aQGIftSjU"},"source":["# Bolsa de Palabras"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3214,"status":"ok","timestamp":1683817304525,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"2LyyJkH4tU9K","outputId":"459716c5-3553-46e2-e3c2-479662bf000d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del vocabulario:  6380\n"]}],"source":["### BOLSA DE PALABRAS ###\n","X_train = X_train.tolist()\n","X_test = X_test.tolist()\n","\n","# entrenamos un modelo de bolsa de palabras\n","bow = CountVectorizer(analyzer=clean_text).fit(X_train)\n","# transformamos el conjunto de entrenamiento a bolsa de palabras\n","X_train_bow = bow.transform(X_train)\n","# transformamos el conjunto de evaluación a bolsa de palabras\n","X_test_bow = bow.transform(X_test)\n","\n","print(\"Tamaño del vocabulario: \", len(bow.vocabulary_))"]},{"cell_type":"markdown","metadata":{"id":"ESXpf0ZAvLAq"},"source":["# TF-IDF"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1683817304527,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"A6kuJq4avM_g"},"outputs":[],"source":["### TF-IDF ###\n","# entrenamos un modelo tf-idf \n","tfidf_transformer = TfidfTransformer().fit(X_train_bow)\n","# transformamos el conjunto de entrenamiento\n","X_train_tfidf = tfidf_transformer.transform(X_train_bow)\n","# transformamos el conjunto de entrenamiento\n","X_test_tfidf = tfidf_transformer.transform(X_test_bow)"]},{"cell_type":"markdown","metadata":{"id":"24Zza6O4wC4q"},"source":["# Clasificación Clásica\n","Se crea un pipeline que ejecuta una secuencia de procesos:\n","\n","\n","1.   La representación de los textos en bolsa de palabras (CountVectorizer), que recibe como entrada los textos, y se les aplica dentro del CountVectorizer la función clean_text para limpiarlos y reducir el ruido. \n","2.   La representación en tf-idf (TfidfTransformer), recibe como entrada la salida del proceso 1, y produce los vectores tf-idf. \n","3. El clasificador SVC, Logistic Regression o Random Forest Clasiffier."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75034,"status":"ok","timestamp":1683817379548,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"-vlh4ZNrwFcH","outputId":"d70a400f-9e1c-4d5a-b596-3d581573014d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 4 folds for each of 8 candidates, totalling 32 fits\n","Los mejores parámetros son : {'svm__C': 1, 'svm__gamma': 1, 'svm__kernel': 'linear'}\n","Mejor accuracy: 0.774\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7f732e544670>)),\n","                ('tf', TfidfTransformer()),\n","                ('svm', SVC(C=1, gamma=1, kernel='linear'))])\n","              precision    recall  f1-score   support\n","\n","           0       0.81      0.89      0.85       361\n","           1       0.71      0.56      0.63       174\n","\n","    accuracy                           0.78       535\n","   macro avg       0.76      0.72      0.74       535\n","weighted avg       0.77      0.78      0.77       535\n","\n"]}],"source":["### PIPELINE SVM ###\n","pipeline = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),  \n","    ('tf', TfidfTransformer()),  \n","    ('svm', SVC()), \n","])\n","\n","# Parámetros para el algoritmo SVM\n","grid_params_svm = [{'svm__kernel': ['linear', 'rbf'], \n","                    'svm__C': [0.1, 1], # [0.1, 1, 10, 100, 1000]\n","                    'svm__gamma':  [1, 0.1] # [1, 0.1, 0.01, 0.001, 0.0001]\n","                    }]\n","gs = GridSearchCV(pipeline, param_grid=grid_params_svm, \n","                  scoring='accuracy', cv=4, verbose = 1)\n","\n","# entrenamos el grid\n","gs.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs.best_params_)\n","print('Mejor accuracy: %.3f' % gs.best_score_)\n","print(gs.best_estimator_)\n","\n","best_svm = gs.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print(classification_report(y_test, predictions))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31757,"status":"ok","timestamp":1683817411292,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"nvM8_10-ajC-","outputId":"d117f33a-41f8-46a5-acfd-661ca6ecb4c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 4 candidates, totalling 20 fits\n","Los mejores parámetros son : {'lr__C': 1.0, 'lr__penalty': 'l2', 'lr__solver': 'liblinear'}\n","Mejor accuracy: 0.761\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7f732e544670>)),\n","                ('tf', TfidfTransformer()),\n","                ('lr', LogisticRegression(random_state=0, solver='liblinear'))])\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.93      0.84       361\n","           1       0.72      0.40      0.52       174\n","\n","    accuracy                           0.76       535\n","   macro avg       0.74      0.66      0.68       535\n","weighted avg       0.75      0.76      0.73       535\n","\n"]}],"source":["### PIPELINE LOGISTIC REGRESSION ###\n","pipeline2 = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),  \n","    ('tf', TfidfTransformer()),  \n","    ('lr', LogisticRegression(random_state=0)), \n","])\n","\n","# Parámetros para el algoritmo Logistic Regression\n","grid_params_lr = [{'lr__penalty': ['l1', 'l2'], \n","                    'lr__C': [1.0, 0.5],\n","                    'lr__solver':  ['liblinear']\n","                    }]\n","gs2 = GridSearchCV(pipeline2, param_grid=grid_params_lr, \n","                  scoring='accuracy', cv=5, verbose = 1)\n","\n","# entrenamos el grid\n","gs2.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs2.best_params_)\n","print('Mejor accuracy: %.3f' % gs2.best_score_)\n","print(gs2.best_estimator_)\n","\n","best_svm = gs2.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print( classification_report(y_test, predictions))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38534,"status":"ok","timestamp":1683817449795,"user":{"displayName":"ALEJANDRO PEREZ AGUADO","userId":"00853298534635197474"},"user_tz":-120},"id":"CMuW5K410K-_","outputId":"fe540c01-a196-43af-ae6c-3e596b90355c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 4 candidates, totalling 20 fits\n","Los mejores parámetros son : {'rfc__criterion': 'gini', 'rfc__max_depth': 10, 'rfc__min_samples_split': 10}\n","Mejor accuracy: 0.677\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7f732e544670>)),\n","                ('tf', TfidfTransformer()),\n","                ('rfc',\n","                 RandomForestClassifier(max_depth=10, min_samples_split=10,\n","                                        random_state=0))])\n","              precision    recall  f1-score   support\n","\n","           0       0.68      1.00      0.81       361\n","           1       1.00      0.01      0.02       174\n","\n","    accuracy                           0.68       535\n","   macro avg       0.84      0.51      0.42       535\n","weighted avg       0.78      0.68      0.55       535\n","\n"]}],"source":["### PIPELINE RANDOM FOREST ###\n","pipeline3 = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),  \n","    ('tf', TfidfTransformer()),  \n","    ('rfc', RandomForestClassifier(random_state=0)), \n","])\n","\n","# Parámetros para el algoritmo Random Forest\n","grid_params_rfc = [{'rfc__criterion': ['gini', 'entropy'], \n","                    'rfc__max_depth': [9, 10],\n","                    'rfc__min_samples_split':  [10]\n","                    }]\n","gs3 = GridSearchCV(pipeline3, param_grid=grid_params_rfc, \n","                  scoring='accuracy', cv=5, verbose = 1)\n","\n","# entrenamos el grid\n","gs3.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs3.best_params_)\n","print('Mejor accuracy: %.3f' % gs3.best_score_)\n","print(gs3.best_estimator_)\n","\n","best_svm = gs3.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print( classification_report(y_test, predictions))"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}